#! /usr/bin/env python3
import time
import os
import logging
import sys
import argparse
import json
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

# Default download path -> ./assets/{book}
C_DIR = os.path.dirname(os.path.realpath(__file__))
DL_PATH = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'assets')

if Path(f'{C_DIR}/config.json').is_file():
    # Read config file to determine if path is set
    with open(f'{C_DIR}/config.json', 'r') as f:
        config = json.load(f)
         
        if config['download_path'] and config['download_path'] != '':
             DL_PATH = config['download_path'].strip()

# Detect download status & time
def dlwait(path):
    ''' Wait until the file download is completed. Will not work if a corrupted download is already 
    in the given path. If the function starts before the download, it won't wait for its completion.'''
    start_time = time.time()
    dl_wait = True
    
    while dl_wait:
        time.sleep(1) # Precision
        dl_wait = False
         
        for book in os.listdir(path):
            if book.endswith('.crdownload'):
                dl_wait = True
     
    runtime = "%s" % round(time.time() - start_time, 4)
    return runtime

# Argparse initialization & parameters
parser = argparse.ArgumentParser()

parser.add_argument('path', metavar='path', type=str, default=DL_PATH, action='store',
                    nargs='?', help='A path for the download')

parser.add_argument('--s', metavar='search', type=str, action='store',
                    required=True, help='A search query for the download')

parser.add_argument('--n', metavar='quantity', type=int, default=5, 
                    help='Number of search results desired')

args = parser.parse_args()

# Disable unsightly webdriver-manager log messages
os.environ['WDM_LOG_LEVEL'] = '0'
os.environ['WDM_LOG'] = str(logging.NOTSET)
logging.getLogger('WDM').setLevel(logging.NOTSET)

chrome_options = Options()

# Include this to remove GUI and extensions for the sake of simplicity
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-logging")
chrome_options.add_argument("--log-level=3")
chrome_options.add_argument("--silent")
chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
chrome_options.add_experimental_option('useAutomationExtension', False)

# Try to get chromedriver path and find the actual executable
try:
    # Get the chromedriver path
    driver_path = ChromeDriverManager().install()
    
    # Fix for the incorrect path issue - find the actual chromedriver executable
    if 'THIRD_PARTY_NOTICES' in driver_path or not driver_path.endswith('chromedriver'):
        # Get the directory containing the chromedriver
        driver_dir = os.path.dirname(driver_path)
        
        # Look for the actual chromedriver executable
        for file in os.listdir(driver_dir):
            if file == 'chromedriver' or (file.startswith('chromedriver') and not file.endswith('.txt')):
                actual_driver_path = os.path.join(driver_dir, file)
                # Make sure it's executable
                os.chmod(actual_driver_path, 0o755)
                driver_path = actual_driver_path
                break
        
        # If we still can't find it, look one level up (sometimes it's in a subdirectory)
        if 'THIRD_PARTY_NOTICES' in driver_path:
            parent_dir = os.path.dirname(driver_dir)
            for root, dirs, files in os.walk(parent_dir):
                for file in files:
                    if file == 'chromedriver':
                        actual_driver_path = os.path.join(root, file)
                        os.chmod(actual_driver_path, 0o755)
                        driver_path = actual_driver_path
                        break
                if file == 'chromedriver':
                    break
    
    driver = webdriver.Chrome(service=ChromeService(driver_path), options=chrome_options)
    
except Exception as e:
    print(f"Error with ChromeDriverManager: {e}")
    print("Trying alternative approach...")
    
    # Use system chromedriver if available
    try:
        # Try to use system chromedriver (if installed via homebrew or manually)
        driver = webdriver.Chrome(options=chrome_options)
    except Exception as e2:
        print(f"System chromedriver also failed: {e2}")
        print("\nPlease try one of these solutions:")
        print("1. Install chromedriver manually:")
        print("   brew install chromedriver (on macOS)")
        print("   or download from https://chromedriver.chromium.org/")
        print("2. Clear the webdriver cache:")
        print("   rm -rf ~/.wdm/")
        print("3. Update your Chrome browser to the latest version")
        sys.exit(1)

DL_PATH = args.path
search = args.s
result_count = args.n

# Ensure download directory exists
os.makedirs(DL_PATH, exist_ok=True)

# Enable downloads in selenium
driver.command_executor._commands["send_command"] = ("POST", '/session/$sessionId/chromium/send_command')

params = {'cmd': 'Page.setDownloadBehavior', 'params': {'behavior': 'allow', 'downloadPath': DL_PATH}}
command_result = driver.execute("send_command", params)

# Array containing search results
sresults = []

# Search query and begin download
start_url = f"https://annas-archive.org/search?q={search}"
driver.get(start_url)

# Wait for page to load
time.sleep(2)

# scroll to the bottom of the page so that every element I search for is returned.
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

# Wait a bit more for dynamic content
time.sleep(1)

# Find book links using the correct selector from the HTML structure
book_links = driver.find_elements(By.CSS_SELECTOR, 'a.js-vim-focus.custom-a')

print(f"Found {len(book_links)} book links")

# Check if we have enough results
if len(book_links) < result_count:
    result_count = len(book_links)
    print(f"Only {result_count} results found")

if result_count == 0:
    print("No results found. Please check your search query.")
    driver.quit()
    sys.exit(1)

# add links to the array of search results
for n in range(result_count):
    href = str(book_links[n].get_attribute('href'))
    sresults.append(href)

# Loop through the book links and extract information directly
for i in range(result_count):
    try:
        # Get the book link element
        book_link = book_links[i]
        
        # Extract title directly from the link text
        title = book_link.text.strip()
        
        # Find the parent container for this book
        # Navigate up to find the main container
        container = book_link
        try:
            for _ in range(5):  # Try up to 5 parent levels
                container = container.find_element(By.XPATH, "..")
                classes = container.get_attribute('class') or ""
                if 'flex' in classes and ('pt-3' in classes or 'border-b' in classes):
                    break
        except:
            # If we can't find the container, use the book_link's immediate parent
            container = book_link.find_element(By.XPATH, "../..")
        
        print(f"\n[{i+1}] {title}")
        
        # Extract all text from the container for debugging
        all_text = container.text
        
        # Extract author - look for any link that contains author info
        author = "Unknown"
        try:
            # Look for links in the container that might be author links
            all_links = container.find_elements(By.TAG_NAME, 'a')
            for link in all_links:
                href = link.get_attribute('href') or ""
                text = link.text.strip()
                # Skip the main title link and empty links
                if href and 'search?q=' in href and text and text != title:
                    # Check if this looks like an author (usually appears after title)
                    if len(text.split(',')) <= 2 and len(text.split()) <= 4:  # Author names are usually short
                        author = text
                        break
        except Exception as e:
            print(f"\t  Error extracting author: {e}")
        
        print(f"\tAuthor: {author}")
        
        # Extract year/publisher from text
        publisher = "Unknown"
        try:
            import re
            # Look for 4-digit year
            year_match = re.search(r'\b(19|20)\d{2}\b', all_text)
            if year_match:
                publisher = year_match.group(0)
        except Exception as e:
            print(f"\t  Error extracting year: {e}")
            
        print(f"\tYear: {publisher}")

        # Extract metadata from text
        language = "Unknown"
        file_format = "Unknown" 
        file_size = "Unknown"
        
        try:
            import re
            
            # Extract language (pattern like "English [en]")
            language_match = re.search(r'(\w+)\s+\[([a-z]{2})\]', all_text)
            if language_match:
                language = language_match.group(1)
            
            # Extract file format (like EPUB, PDF) - look for common formats
            format_match = re.search(r'\b(EPUB|PDF|MOBI|AZW3|TXT|DOC|DOCX)\b', all_text, re.IGNORECASE)
            if format_match:
                file_format = format_match.group(1).upper()
            
            # Extract file size (like 0.3MB)
            size_match = re.search(r'(\d+\.?\d*\s*[MKG]B)', all_text, re.IGNORECASE)
            if size_match:
                file_size = size_match.group(1)
                
        except Exception as e:
            print(f"\t  Error extracting metadata: {e}")

        print(f"\tLanguage: {language}")
        print(f"\tFormat: {file_format}, Size: {file_size}")
        
        # Debug: print some of the raw text if needed
        if i == 0:  # Print debug info for first result
            print(f"\t  Debug - Container text preview: {all_text[:200]}...")
        
    except Exception as e:
        print(f"\n[{i+1}] Error extracting info for book {i+1}: {e}")
        # Print at least the title if we can get it
        try:
            title = book_links[i].text.strip()
            print(f"\tTitle: {title}")
        except:
            print(f"\tCould not extract title")

try:
    book_selection = int(input("\nEnter desired download: "))
    
    if book_selection < 1 or book_selection > len(sresults):
        print("Invalid selection")
        driver.quit()
        sys.exit(1)

    driver.get(sresults[book_selection-1])
    
    # Wait for page to load
    time.sleep(2)

    # click "show external downloads" - try multiple possible selectors
    try:
        show_link = driver.find_element(By.XPATH, "//a[contains(text(), 'show external downloads')]")
        show_link.click()
    except NoSuchElementException:
        try:
            show_link = driver.find_element(By.XPATH, "//a[contains(text(), 'external downloads')]")
            show_link.click()
        except NoSuchElementException:
            try:
                show_link = driver.find_element(By.XPATH, "//button[contains(text(), 'show external downloads')]")
                show_link.click()
            except NoSuchElementException:
                print("Could not find 'show external downloads' link. Looking for direct download links...")

    # Wait for external downloads to load
    time.sleep(2)

    page_text = driver.find_element(By.XPATH, "/html/body").text.lower()

    if 'libgen' in page_text:
        try:
            # Try multiple selectors for libgen link
            libgen_link = None
            
            # Try different possible selectors
            selectors = [
                "//a[@class='js-download-link' and contains(@href, 'libgen') and text()='Libgen.li']",
                "//a[contains(@href, 'libgen') and contains(@class, 'download')]",
                "//a[contains(text(), 'Libgen') and contains(@href, 'libgen')]",
                "//a[contains(@href, 'libgen')]"
            ]
            
            for selector in selectors:
                try:
                    element = driver.find_element(By.XPATH, selector)
                    libgen_link = element.get_attribute('href')
                    break
                except NoSuchElementException:
                    continue
            
            if not libgen_link:
                print("Could not find libgen download link")
                raise NoSuchElementException("Libgen link not found")

            # Open download link on libgen
            driver.get(libgen_link)
            time.sleep(3)
            
            # Look for download link on libgen page
            try:
                driver.execute_script("document.querySelector('a[href*=\"get.php\"]').click();")
            except:
                # Try alternative selectors
                download_selectors = [
                    "//a[contains(@href, 'get.php')]",
                    "//a[contains(text(), 'GET')]",
                    "//a[contains(@href, 'download')]"
                ]
                
                for selector in download_selectors:
                    try:
                        download_link = driver.find_element(By.XPATH, selector)
                        download_link.click()
                        break
                    except NoSuchElementException:
                        continue

            print(f"Downloading ...")

            dl_time = dlwait(DL_PATH)

            print(f'Successfully downloaded. ({dl_time} seconds)')
            
        except Exception as e:
            print(f"Error with libgen download: {e}")
            print("Falling back to manual link selection...")
    
    if 'libgen' not in page_text or 'Error with libgen download:' in str(locals()):
        print("ERROR: libgen.li link not found or failed.")
        print("The following links require human verification.")
        time.sleep(2)

        # Find all download links
        try:
            # Try to find download links in various formats
            download_elements = driver.find_elements(By.CSS_SELECTOR, 'a.js-download-link')
            
            if not download_elements:
                download_elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*="download"]')
            
            if not download_elements:
                download_elements = driver.find_elements(By.XPATH, "//a[contains(@href, 'http') and (contains(@href, 'libgen') or contains(@href, 'download') or contains(@href, 'mirror'))]")

            # Iterate through the download options
            for i, element in enumerate(download_elements, start=1):
                try:
                    link_text = element.text.strip()
                    link_href = element.get_attribute('href')
                    
                    # Get parent element text for context
                    parent_text = ""
                    try:
                        parent_text = element.find_element(By.XPATH, "..").text.strip()
                    except:
                        pass

                    # Print the extracted information
                    print(f"\nOption {i}:")
                    print(f"\tLink Text: {link_text}")
                    print(f"\tLink Href: {link_href}")
                    if parent_text and parent_text != link_text:
                        print(f"\tContext: {parent_text}")

                except Exception as e:
                    print(f"\nOption {i}: Error extracting link info - {e}")
                    
        except Exception as e:
            print(f"Could not find download links: {e}")

except KeyboardInterrupt:
    print("\nScript interrupted by user")
except Exception as e:
    print(f"An error occurred: {e}")
finally:
    driver.quit()
